import os
import shutil
from random import shuffle


def init_lib(lib_name):
    with open(lib_name) as f:
        functions = f.read()
        splitted = functions.split('\n')

    lib = dict(zip(splitted, range(1, len(splitted) + 1)))
    return lib


def raw_process(log):
    log = log.replace('\x00', '')
    nline = log.replace('\r', '').split('\n')

    branch = filter(lambda x: '+' in x, nline)
    return list(branch)


def api_parser(rows):
    splitted = map(lambda x: x[1:].split(','), rows)
    api_filtering = filter(lambda x: len(x) == 4, splitted)

    apis = map(lambda x: x[3], api_filtering)
    filt = filter(lambda x: x != '', apis)

    return list(filt)


def map_index(rows, lib):
    mapped = map(lambda x: lib[x] if x in lib else 0, rows)
    return list(mapped)


def pattern_reducer(apis, max_len, min_len):
    size = 1
    while size < len(apis) / 2:
        ptr = 0
        while ptr < len(apis) - size * 2:
            is_pattern = True
            n = 0
            while n < size:
                if apis[ptr + n] != apis[ptr + size + n]:
                    is_pattern = False
                    break
                n += 1

            if is_pattern:
                end = ptr + size + size

                len_apis = len(apis)
                while is_pattern and end + size < len_apis:
                    n = 0
                    while n < size:
                        if apis[ptr + n] != apis[end + n]:
                            is_pattern = False
                            break
                        n += 1
                    end += size

                if not is_pattern:
                    end -= size

                del apis[ptr + size:end]
                apis.insert(ptr + size, -1)
            ptr += 1
        size += 1

    apis = list(filter(lambda x: x != -1, apis))

    if len(apis) > min_len:
        if len(apis) > max_len:
            apis = apis[:max_len]

        return apis
    else:
        raise ValueError('Log length is too short.')


def pre_process(name,
                label,
                max_len=100,
                min_len=5,
                lib=None,
                lib_name='./data/function_list.txt'):
    if lib is None:
        lib = init_lib(lib_name)

    with open(name) as f:
        log = f.read()

        data = raw_process(log)
        parse = api_parser(data)

        if parse != '':
            indexed = map_index(parse, lib)
            reduced = pattern_reducer(indexed, max_len, min_len)
            processed = [label] + reduced

    return processed


def proc_dir(dir_name,
             label,
             max_len=100,
             min_len=5,
             lib=None,
             lib_name='./data/function_list.txt'):
    if lib is None:
        lib = init_lib(lib_name)

    sample_name = os.listdir(dir_name)
    samples = []

    for name in sample_name:
        try:
            log_name = os.path.join(dir_name, name)
            data = pre_process(log_name, label, max_len, min_len, lib)
            samples.append(data)
        except ValueError:
            pass

    return samples


def make_csv(mal_size,
             norm_size,
             max_len=100,
             min_len=5
             mal_dir='./log/malware',
             norm_dir='./log/normal',
             save_dir='./data',
             mal_name='mal_trainset.csv',
             norm_name='norm_trainset.csv',
             set_name='testset.csv',
             lib=None,
             lib_name='./data/function_list.txt'):
    if lib is None:
        lib = init_lib(lib_name)

    malware_samples = proc_dir(mal_dir, 1, max_len, min_len, lib)
    normal_samples = proc_dir(norm_dir, 0, max_len, min_len, lib)

    malware_set = list(map(lambda x: list(map(str, x)), malware_samples))
    normal_set = list(map(lambda x: list(map(str, x)), normal_samples))

    shuffle(malware_set)
    shuffle(normal_set)

    def comma_binder(x):
        return ','.join(x)

    with open(os.path.join(save_dir, mal_name), 'w') as f:
        malware = map(comma_binder, malware_set[:mal_size])
        trainset = '\n'.join(malware)
        f.write(trainset)

    with open(os.path.join(save_dir, norm_name), 'w') as f:
        normal = map(comma_binder, normal_set[:norm_size])
        trainset = '\n'.join(normal)
        f.write(trainset)

    with open(os.path.join(save_dir, set_name), 'w') as f:
        malware = map(comma_binder, malware_set[mal_size:])
        normal = map(comma_binder, normal_set[norm_size:])
        testset = '\n'.join(malware) + '\n' + '\n'.join(normal)

        f.write(testset)
