import os
import sys
import numpy as np
import tensorflow as tf

from ml.t0_virus_class import FLAGS, get_data, make_model

lib = {}
path = os.path.join(FLAGS.train_dir, 'function_list.txt')
with open(path) as f:
    functions = f.read()
    splitted = functions.split('\n')

    lib = dict(zip(splitted, range(1, len(splitted) + 1)))

def make_index(name):
    if name in lib:
        return lib[name]
    else:
        return 0

def pattern_reducer(apis):
    size = 1
    while size < len(apis) / 2:
        ptr = 0
        while ptr < len(apis) - size * 2:
            is_pattern = True
            n = 0
            while n < size:
                if apis[ptr + n] != apis[ptr + size + n]:
                    is_pattern = False
                    break
                n += 1

            if is_pattern:
                end = ptr + size + size

                len_apis = len(apis)
                while is_pattern and end + size < len_apis:
                    n = 0
                    while n < size:
                        if apis[ptr + n] != apis[end + n]:
                            is_pattern = False
                            break
                        n += 1
                    end += size

                if not is_pattern:
                    end -= size

                del apis[ptr + size:end]
                apis.insert(ptr + size, -1)
            ptr += 1
        size += 1

    return filter(lambda x: x != -1, apis)

def scale_branch_feature(data, seq_len = FLAGS.seq_len, head_ignore = True):
    rows = data.split('\n')
    if head_ignore:
        rows = rows[1:]

    splitted = map(lambda x: x.split(','), rows)
    filtered = filter(lambda x: len(x) == 4, splitted)
    tmp_apis = map(lambda x: x[3], filtered)
    apis = filter(lambda x: x != '', tmp_apis)

    apis_numbering = map(make_index, apis)
    apis_scaled = pattern_reducer(apis_numbering)

    if len(apis_scaled) < 5:
        return []
    elif len(apis_scaled) > 100:
        apis_scaled = apis_scaled[:100]

    padd = [[0]] * (seq_len - len(apis_scaled))
    return np.array(map(lambda x: [x], apis_scaled) + padd)

def predict_with_log_data(filename,
                        train_dir = FLAGS.train_dir,
                        seq_len = FLAGS.seq_len,
                        num_hidden = FLAGS.num_hidden):
    with open(filename) as f:
        data = f.read()

    scale = scale_branch_feature(data)
    X, _, prediction, _, _ = make_model(seq_len, num_hidden)

    saver = tf.train.Saver()
    save_path = os.path.join(train_dir, 'train')

    with tf.Session() as sess:
        ckpt = tf.train.get_checkpoint_state(save_path)
        if ckpt and ckpt.model_checkpoint_path:
            saver.restore(sess, ckpt.model_checkpoint_path)
        else:
            return []

        result = sess.run(prediction, feed_dict = {X: [scale]})

    return result

def error_rate_with_dataset(dataset = FLAGS.dataset,
                            train_dir = FLAGS.train_dir,
                            seq_len = FLAGS.seq_len,
                            num_hidden = FLAGS.num_hidden):
    test_x, test_y, seq_len = get_data(train_dir, dataset, seq_len)
    X, Y, _, err, _ = make_model(seq_len, num_hidden)

    saver = tf.train.Saver()
    save_path = os.path.join(train_dir, 'train')

    with tf.Session() as sess:
        ckpt = tf.train.get_checkpoint_state(save_path)
        if ckpt and ckpt.model_checkpoint_path:
            saver.restore(sess, ckpt.model_checkpoint_path)
        else:
            return 0

        errors = sess.run(err, feed_dict = {X: test_x, Y: test_y})

    return errors

def main(argv):
    if len(argv) < 2:
        print 'USAGE : error_dataset | predict_log FILENAME [options]'
        return 1

    if argv[1] == 'error_dataset':
        err = error_rate_with_dataset()
        print 'path : %s, error rate : %.1f' \
                    % (os.path.join(FLAGS.train_dir, FLAGS.dataset), err * 100)

    elif argv[1] == 'predict_log':
        if len(argv) < 3:
            print 'required log filename'
            return 1

        pred = predict_with_log_data(argv[2])
        print 'path : %s, predict result : %.1f' % (argv[2], pred[0][0] * 100)

    else:
        print 'unsupported commands'
        return1

    return 0

if __name__ == '__main__':
    main(sys.argv)
